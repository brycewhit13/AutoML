{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Started: training an image classification model\n",
        "\n",
        "**Learning Objectives** - By the end of this quickstart tutorial, you'll know how to train and deploy an image classification model on Azure Machine Learning studio.\n",
        "\n",
        "This tutorial covers:\n",
        "\n",
        "- Connect to workspace & set up a compute resource on the Azure Machine Learning Studio Notebook UI\n",
        "- Bring data in and prepare it to be used for training\n",
        "- Train a model for image classification\n",
        "- Metrics for optimizing your model\n",
        "- Deploy the model online & test"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Connect to Azure Machine Learning workspace\n",
        "\n",
        "Before we dive in the code, you'll need to connect to your workspace. The workspace is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning.\n",
        "\n",
        "We are using `DefaultAzureCredential` to get access to workspace. `DefaultAzureCredential` should be capable of handling most scenarios. If you want to learn more about other available credentials, go to [set up authentication doc](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-setup-authentication?tabs=sdk), [azure-identity reference doc](https://learn.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python).\n",
        "\n",
        "**Make sure to enter your workspace credentials before you run the script below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle to the workspace\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "# Get a handle to the workspace. You can find the info on the workspace tab on ml.azure.com\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",  # this will look like xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n",
        "    resource_group_name=\"DefaultResourceGroup-EUS2\",\n",
        "    workspace_name=\"chest-xrays-automl\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Create compute\n",
        "\n",
        "In order to train a model on the Notebook editor on Azure Machine Learning studio, you will need to create a compute resource first. This is easily handled through a compute creation wizard. **Creating a compute will take 3-4 minutes.**\n",
        "\n",
        "![](media/compute-creation.png)\n",
        "\n",
        "1. Click **...** menu button on the top of Notebook UI, and select **+Create Azure ML Compute Instance**.\n",
        "2. **Name** the compute as **cpu-cluster**\n",
        "3. Select **CPU** and **STANDARD_DS3_V2**. \n",
        "4. Click **Create**\n",
        "\n",
        "If you are interested in learning how to create compute via code, see [Azure Machine Learning in a Day](https://github.com/Azure/azureml-examples/blob/main/tutorials/azureml-in-a-day/azureml-in-a-day.ipynb). "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Create a job environment\n",
        "To run an Azure Machine Learning training job, you'll need an environment.\n",
        "\n",
        "In this tutorial, you'll using a ready-made environment called `AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest` that contains all required libraries (python, MLflow, numpy, pip, etc). "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Build the command job to train\n",
        "Now that you have all assets required to run your job, it's time to build the job itself, using the Azure ML Python SDK v2. We will be creating a command job.\n",
        "\n",
        "An AzureML command job is a resource that specifies all the details needed to execute your training code in the cloud: inputs and outputs, the type of hardware to use, software to install, and how to run your code. the command job contains information to execute a single command.\n",
        "\n",
        "**Create training script**\n",
        "\n",
        "Let's start by creating the training script - the *main.py* python file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "train_src_dir = \"./src\"\n",
        "os.makedirs(train_src_dir, exist_ok=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This script handles the preprocessing of the data, splitting it into test and train data. It then consumes this data to train a tree based model and return the output model. [MLFlow](https://mlflow.org/docs/latest/tracking.html) will be used to log the parameters and metrics during our pipeline run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ./src/main.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile {train_src_dir}/main.py\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import mlflow\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "def train_model(model,criterion,optimizer,loader,n_epochs,device):\n",
        "    \n",
        "    loss_over_time = [] # to track the loss as the network trains\n",
        "    \n",
        "    model = model.to(device) # Send model to GPU if available\n",
        "    model.train() # Set the model to training mode\n",
        "    \n",
        "    for epoch in range(n_epochs):  # loop over the dataset multiple times\n",
        "        \n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0\n",
        "        \n",
        "        for i, data in enumerate(loader):\n",
        "            # Get the input images and labels, and send to GPU if available\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            # Convert to one channel image (grayscale)\n",
        "            inputs = inputs[:,0,:,:].unsqueeze(1)\n",
        "\n",
        "            # Zero the weight gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass to get outputs\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.argmax(outputs, 1)\n",
        "\n",
        "            # Calculate the loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backpropagation to get the gradients with respect to each weight\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Convert loss into a scalar and add it to running_loss\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Convert loss into a scalar and add it to running_loss\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            # Track number of correct predictions\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "            \n",
        "        # Calculate and display average loss and accuracy for the epoch\n",
        "        epoch_loss = running_loss / len(loader)\n",
        "        epoch_acc = running_corrects.double() / len(loader)\n",
        "        print('Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "        loss_over_time.append(epoch_loss)\n",
        "\n",
        "    return loss_over_time\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function of the script.\"\"\"\n",
        "    print('Starting...')\n",
        "    # input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
        "    args = parser.parse_args()\n",
        "   \n",
        "    # start Logging\n",
        "    mlflow.start_run()\n",
        "\n",
        "    ###################\n",
        "    #<prepare the data>\n",
        "    ###################\n",
        "    train_path = os.path.join(args.data, 'train')\n",
        "    test_path = os.path.join(args.data, 'test')\n",
        "    \n",
        "    # Create datasets    \n",
        "    train_data = torchvision.datasets.ImageFolder(train_path, transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0.5],std=[0.5])]))\n",
        "    test_data = torchvision.datasets.ImageFolder(test_path, transform=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(),transforms.Normalize(mean=[0.5],std=[0.5])]))\n",
        "    \n",
        "    # Create dataloader\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=False)\n",
        "    \n",
        "    ###################\n",
        "    #</prepare the data>\n",
        "    ###################\n",
        "\n",
        "    ##################\n",
        "    #<train the model>\n",
        "    ##################\n",
        "    \n",
        "    model = torchvision.models.resnet18(pretrained=True)\n",
        "    \n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "        \n",
        "    # Replace the resnet input layer to take in grayscale images (1 input channel), since it was originally trained on color (3 input channels)\n",
        "    in_channels = 1\n",
        "    model.conv1 = torch.nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "    # Replace the resnet final layer with a new fully connected Linear layer we will train on our task\n",
        "    # Number of out units is number of classes (3)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = torch.nn.Linear(num_ftrs, 3)\n",
        "    \n",
        "    # Set device\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Train the model\n",
        "    n_epochs = 10\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
        "    cost_path_train = train_model(model,criterion,optimizer,train_dataloader,n_epochs,device)\n",
        "    \n",
        "    # Test the model\n",
        "    cost_path_test = train_model(model,criterion,optimizer,test_dataloader,n_epochs,device)\n",
        "    \n",
        "    # Print the train and test loss\n",
        "    print(f\"Train loss: {cost_path_train[-1]}\")\n",
        "    print(f\"Test loss: {cost_path_test[-1]}\")\n",
        "    \n",
        "    ##################\n",
        "    #</train the model>\n",
        "    ##################\n",
        "\n",
        "    ##########################\n",
        "    #<save and register model>\n",
        "    ##########################\n",
        "    # registering the model to the workspace\n",
        "    print(\"Registering the model via MLFlow\")\n",
        "    mlflow.pytorch.log_model(model, args.registered_model_name)\n",
        "\n",
        "    # saving the model to a file\n",
        "    mlflow.pytorch.save_model(model, path='latest_model')\n",
        "    \n",
        "    # stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see in this script, once the model is trained, the model file is saved and registered to the workspace. Now you can use the registered model in inferencing endpoints."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Configure the Command**\n",
        "\n",
        "Now that you have a script that can perform the desired tasks, You'll use the general purpose command that can run command line actions. This command line action can be directly calling system commands or running a script.\n",
        "\n",
        "Here, you'll use input data, split ratio, learning rate and registered model name as input variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import the libraries\n",
        "from azure.ai.ml import command, Input\n",
        "\n",
        "# name the model you registered earlier in the training script\n",
        "registered_model_name = \"chest_xray_classifier\"\n",
        "\n",
        "# configure the command job\n",
        "job = command(\n",
        "    inputs=dict(\n",
        "        # uri_file refers to a specific file as a data asset\n",
        "        data='chest_xrays',  \n",
        "        registered_model_name=registered_model_name,  # input variable in main.py\n",
        "    ),\n",
        "    code=\"./\",  # location of source code\n",
        "    # The inputs/outputs are accessible in the command via the ${{ ... }} notation\n",
        "    command=\"python src/main.py --data ${{inputs.data}}  --registered_model_name ${{inputs.registered_model_name}}\",\n",
        "    # This is the ready-made environment you are using\n",
        "    #environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
        "    environment=\"AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu@latest\",\n",
        "    # This is the compute you created earlier\n",
        "    compute=\"chest-xray-compute\",\n",
        "    # An experiment is a container for all the iterations one does on a certain project. All the jobs submitted under the same experiment name would be listed next to each other in Azure ML studio.\n",
        "    experiment_name=\"automated_training_experiment\",\n",
        "    display_name=\"automated_training_run\",\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Submit the job ###\n",
        "It's now time to submit the job to run in AzureML. **The job will take 2 to 3 minutes to run**. It could take longer (up to 10 minutes) if the compute instance has been scaled down to zero nodes and custom environment is still building."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n",
            "\n",
            "Example: azcopy copy 'C:\\Duke\\AIPI561\\Projects\\Individual_Project_2\\AutoML' 'https://chestxraysauto8122485356.blob.core.windows.net/c5e88b7e-1-e7a43580-b93b-5e87-9420-302122b7802d/AutoML' \n",
            "\n",
            "See https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n",
            "\u001b[32mUploading AutoML (332.25 MBs): 100%|##########| 332249803/332249803 [03:47<00:00, 1461324.80it/s]\n",
            "\u001b[39m\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>automated_training_experiment</td><td>frosty_map_58vtrbz7kd</td><td>command</td><td>Starting</td><td><a href=\"https://ml.azure.com/runs/frosty_map_58vtrbz7kd?wsid=/subscriptions/9b87d29a-2963-4e02-b31d-8d08dba1102d/resourcegroups/DefaultResourceGroup-EUS2/workspaces/chest-xrays-automl&amp;tid=cb72c54e-4a31-4d9e-b14a-1ea36dfac94c\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
            ],
            "text/plain": [
              "Command({'parameters': {}, 'init': False, 'name': 'frosty_map_58vtrbz7kd', 'type': 'command', 'status': 'Starting', 'log_files': None, 'description': None, 'tags': {}, 'properties': {'mlflow.source.git.repoURL': 'https://github.com/brycewhit13/AutoML.git', 'mlflow.source.git.branch': 'main', 'mlflow.source.git.commit': 'a36457bbe7a161b0e920c53bf5af32acb66604fb', 'azureml.git.dirty': 'True', '_azureml.ComputeTargetType': 'amlcdsi', 'ContentSnapshotId': '816c3499-6f79-40e0-a094-e9703b110f4b'}, 'print_as_yaml': True, 'id': '/subscriptions/9b87d29a-2963-4e02-b31d-8d08dba1102d/resourceGroups/DefaultResourceGroup-EUS2/providers/Microsoft.MachineLearningServices/workspaces/chest-xrays-automl/jobs/frosty_map_58vtrbz7kd', 'Resource__source_path': None, 'base_path': 'c:\\\\Duke\\\\AIPI561\\\\Projects\\\\Individual_Project_2\\\\AutoML', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x000002679C53ED90>, 'serialize': <msrest.serialization.Serializer object at 0x000002679C53E040>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': 'automated_training_run', 'experiment_name': 'automated_training_experiment', 'compute': 'chest-xray-compute', 'services': {'Tracking': {'endpoint': 'azureml://eastus2.api.azureml.ms/mlflow/v1.0/subscriptions/9b87d29a-2963-4e02-b31d-8d08dba1102d/resourceGroups/DefaultResourceGroup-EUS2/providers/Microsoft.MachineLearningServices/workspaces/chest-xrays-automl?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/frosty_map_58vtrbz7kd?wsid=/subscriptions/9b87d29a-2963-4e02-b31d-8d08dba1102d/resourcegroups/DefaultResourceGroup-EUS2/workspaces/chest-xrays-automl&tid=cb72c54e-4a31-4d9e-b14a-1ea36dfac94c', 'type': 'Studio'}}, 'comment': None, 'job_inputs': {'data': 'chest_xrays', 'registered_model_name': 'chest_xray_classifier'}, 'job_outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.frosty_map_58vtrbz7kd', 'mode': 'rw_mount'}}, 'inputs': {'data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000002679C53E400>, 'registered_model_name': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x000002679C53E4F0>}, 'outputs': {'default': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x000002679C52ED00>}, 'component': CommandComponent({'intellectual_property': None, 'auto_increment_version': True, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': False, 'auto_delete_setting': None, 'name': 'frosty_map_58vtrbz7kd', 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': WindowsPath('.'), 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x000002679C53ED90>, 'serialize': <msrest.serialization.Serializer object at 0x000002679C53EBB0>, 'command': 'python src/main.py --data ${{inputs.data}}  --registered_model_name ${{inputs.registered_model_name}}', 'code': '/subscriptions/9b87d29a-2963-4e02-b31d-8d08dba1102d/resourceGroups/DefaultResourceGroup-EUS2/providers/Microsoft.MachineLearningServices/workspaces/chest-xrays-automl/codes/1dbce2d8-6253-4edf-bcb6-894b45e4dae1/versions/1', 'environment_variables': {}, 'environment': 'azureml:AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu@latest', 'distribution': None, 'resources': None, 'queue_settings': None, 'version': None, 'latest_version': None, 'schema': None, 'type': 'command', 'display_name': 'automated_training_run', 'is_deterministic': True, 'inputs': {'data': {'type': 'string', 'default': 'chest_xrays'}, 'registered_model_name': {'type': 'string', 'default': 'chest_xray_classifier'}}, 'outputs': {'default': {'type': 'uri_folder', 'path': 'azureml://datastores/workspaceartifactstore/ExperimentRun/dcid.frosty_map_58vtrbz7kd', 'mode': 'rw_mount'}}, 'yaml_str': None, 'other_parameter': {'status': 'Starting', 'parameters': {}}, 'additional_includes': [], 'CommandComponent__additional_includes_obj': None}), 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': {'Tracking': {'endpoint': 'azureml://eastus2.api.azureml.ms/mlflow/v1.0/subscriptions/9b87d29a-2963-4e02-b31d-8d08dba1102d/resourceGroups/DefaultResourceGroup-EUS2/providers/Microsoft.MachineLearningServices/workspaces/chest-xrays-automl?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/frosty_map_58vtrbz7kd?wsid=/subscriptions/9b87d29a-2963-4e02-b31d-8d08dba1102d/resourcegroups/DefaultResourceGroup-EUS2/workspaces/chest-xrays-automl&tid=cb72c54e-4a31-4d9e-b14a-1ea36dfac94c', 'type': 'Studio'}}, 'status': 'Starting', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x000002679C53ED90>}, 'instance_id': '47ef6da7-83d0-403c-9e1d-a5dfc26e0dc2', 'source': 'BUILDER', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': 'azureml:AzureML-pytorch-1.10-ubuntu18.04-py38-cuda11-gpu@latest', 'resources': {'instance_count': 1, 'shm_size': '2g'}, 'queue_settings': None, 'swept': False})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# submit the command job\n",
        "ml_client.create_or_update(job)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. View the result of a training job"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](media/view-job.gif)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "You can view the result of a training job by **clicking the URL generated after submitting a job**. Alternatively, you can also click **Jobs** on the left navigation menu. A job is a grouping of many runs from a specified script or piece of code. Information for the run is stored under that job. \n",
        "\n",
        "1. **Overview** is where you can see the status of the job.  \n",
        "2. **Metrics** would display different visualizations of the metrics you specified in the script.\n",
        "3. **Images** is where you can view any image artifacts that you have logged with MLflow.\n",
        "4. **Child jobs** contains child jobs if you added them.\n",
        "5. **Outputs + logs** contains log files you need for troubleshooting or other monitoring purposes. \n",
        "6. **Code** contains the script/code used in the job.\n",
        "7. **Explanations** and **Fairness** are used to see how your model performs against responsible AI standards. They are currently preview features and require additional package installations.\n",
        "8. **Monitoring** is where you can view metrics for the performance of compute resources.  \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Deploy the model as an online endpoint\n",
        "\n",
        "Now deploy your machine learning model as a web service in the Azure cloud, an [`online endpoint`](https://docs.microsoft.com/azure/machine-learning/concept-endpoints).\n",
        "\n",
        "To deploy a machine learning service, you usually need:\n",
        "\n",
        "- The model assets (file, metadata) that you want to deploy. You've already registered these assets via MLflow in *main.py*. You can find it under **Models** on the left navigation menu on Azure Machine Learning studio. \n",
        "- The code that executes the model on a given input request. In this quickstart, you can easily set it up through the endpoint creation UI. If you want to learn more about how to deploy via Azure Machine Learning SDK, see [Azure Machine Learning in a Day](https://github.com/Azure/azureml-examples/blob/main/tutorials/azureml-in-a-day/azureml-in-a-day.ipynb)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](media/endpoint-creation.gif)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Find the endpoint creation wizard on Studio**\n",
        "1. Open a duplicate tab (so that you can keep this tutorial open).\n",
        "1. On the duplicate tab, select **Endpoints** on the left navigation menu.\n",
        "2. Select **+Create** for real-time endpoints.\n",
        "\n",
        "**Endpoint creation & deployment via wizard UI** (this will take approximately 6 to 8 minutes)\n",
        "1. Enter a **unique name** for *endpoint name*. We recommend creating a *unique* name with current date/time to avoid conflicts, which could prevent your deployment. Keep all the defaults for the rest. \n",
        "2. Next, you need to choose a model to deploy. Select **credit_defaults_model** registered by *main.py* earlier. \n",
        "3. Keep all the defaults for deployment configuration.\n",
        "1. Select **Standard_DS3_V2** for compute, which is what we configured earlier. Set the instance count to **1**.\n",
        "1. Keep all the defaults for the traffic.\n",
        "1. Review: review and select **Create**.  \n",
        "\n",
        "![](media/endpoint-test.gif)\n",
        "\n",
        "**Test with a sample query**\n",
        "1. Select the endpoint you just created. Make sure the endpoint is created and the model has been deployed to it.\n",
        "2. Select the **Test** tab.\n",
        "3. Copy & paste the following sample request file into the **Input data to test real-time endpoint** field.\n",
        "4. Select **Test**. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "{\n",
        "  \"input_data\": {\n",
        "    \"columns\": [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],\n",
        "    \"index\": [0, 1],\n",
        "    \"data\": [\n",
        "            [20000,2,2,1,24,2,2,-1,-1,-2,-2,3913,3102,689,0,0,0,0,689,0,0,0,0],\n",
        "            [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8]\n",
        "        ]\n",
        "  }\n",
        "}\n",
        "```"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Clean up resources**\n",
        "\n",
        "If you're not going to use the endpoint, delete it to stop using the resource. Make sure no other deployments are using an endpoint before you delete it.\n",
        "\n",
        "1. Click **Details** on the endpoint page.\n",
        "2. Click the **Delete** button.\n",
        "\n",
        "**Expect this step to take approximately 6 to 8 minutes.**"
      ]
    }
  ],
  "metadata": {
    "categories": [
      "SDK v2",
      "tutorials"
    ],
    "description": {
      "description": "A quickstart tutorial to train and deploy an image classification model on Azure Machine Learning studio"
    },
    "kernelspec": {
      "display_name": "automl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
